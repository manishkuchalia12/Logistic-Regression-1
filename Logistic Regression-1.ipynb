{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b0572b-8bf9-4ebc-83c2-70cde9f6c347",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of \n",
    "a scenario where logistic regression would be more appropriate.\n",
    "Ans:-Linear Regression:\n",
    "\r\n",
    "Linear regression is a supervised machine learning algorithm used for predicting a continuous outcome variable (also known as the dependent variable) based on one or more independent variables. The relationship between the variables is modeled as a linear equation, and the algorithm aims to find the best-fitting line that minimizes the sum of squared error\n",
    "Key Differences:\r\n",
    "\r\n",
    "Outcome Variable:\r\n",
    "\r\n",
    "Linear regression predicts a continuous outcome variable.\r\n",
    "Logistic regression predicts the probability of an instance belonging to a specific class (binary classification).\r\n",
    "Equation Form:\r\n",
    "\r\n",
    "Linear regression uses a linear equation.\r\n",
    "Logistic regression uses the logistic (sigmoid) function to model probabilities.\r\n",
    "Outinfinity to -infinity\n",
    "−∞ to \r\n",
    "+\r\n",
    "∞\r\n",
    "+∞.\r\n",
    "Logistic regression output ranges from 0 to 1 (representing probabiliies).\r\n",
    "Application:\r\n",
    "\r\n",
    "Linear regression is suitable for regression problems, such as predicting house prices, stock prices, or temperature.\r\n",
    "Logistic regression is suitable for binary classification problems, like predicting whether an email is spam or not, whether a patient has a particular disease, etc.\r\n",
    "Scenario or Logistic Regression:\r\n",
    "\r\n",
    "Let's consider an example scenario where logistic regresion is more appropriate:\r\n",
    "\r\n",
    "Scenario: redicting Student Admission\r\n",
    "\r\n",
    "Problem Type: Binary classification\r\n",
    "Objective: Predict whether a student is admitted (1) or not admitted (0) to a university based on their exam scores.\r\n",
    "Features: Exam scores (independent variable)\r\n",
    "Outcome: Admission status (0 or 1)s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3240d29-f9af-4234-9297-0c7a3c2ab540",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cf4cc-48ef-4f3d-8c5d-23b8db82c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = (4 + 3 * X + np.random.randn(100, 1) > 5).astype(int)\n",
    "\n",
    "# Add bias term to the features\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Initialize model parameters\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Logistic function (sigmoid)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic loss (binary cross-entropy)\n",
    "def logistic_loss(y, y_pred):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, theta, learning_rate, epochs):\n",
    "    m = len(y)\n",
    "    for epoch in range(epochs):\n",
    "        logits = X.dot(theta)\n",
    "        y_pred = sigmoid(logits)\n",
    "        loss = logistic_loss(y, y_pred)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = X.T.dot(y_pred - y) / m\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        theta -= learning_rate * gradients\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "theta_optimized = gradient_descent(X_b, y, theta, learning_rate, epochs)\n",
    "\n",
    "# Display the optimized parameters\n",
    "print('Optimized Parameters:')\n",
    "print('Theta_0:', theta_optimized[0][0])\n",
    "print('Theta_1:', theta_optimized[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479da3ab-6fd8-4563-b702-5ca87f2b4763",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5611c75-3add-45f0-9010-41d6638d8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L2 regularization (Ridge)\n",
    "# The parameter C is the inverse of the regularization strength\n",
    "# Smaller C values result in stronger regularization\n",
    "model = LogisticRegression(penalty='l2', C=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8944919-58da-4f1e-a61d-c4de3fa5c34e",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression \n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c9cea-1163-4c73-877f-da9220e767b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c0b7e-dd6e-4e17-a388-4799029c826e",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these \n",
    "techniques help improve the model's performance?\n",
    "Ans:-Feature selection is a process of choosing a subset of relevant features to use in the model. In logistic regression, selecting the most informative features can lead to a simpler and more interpretable model, reduce the risk of overfitting, and potentially improve the model's performance. Here are some common techniques for feature selection in logistic regression:\r",
    "\r\n",
    "1. Recursive Feature Elimination (RFE):\r\n",
    "RFE recursively removes the least important features based on their weights and evaluates the model performance at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfc23c-de0d-491d-9368-006f34db42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Initialize RFE with the logistic regression model\n",
    "rfe = RFE(model, n_features_to_select=5)  # Choose the desired number of features\n",
    "\n",
    "# Fit RFE and get the selected features\n",
    "X_selected = rfe.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd12b6-dac8-47de-b621-a5ffa82cd8ac",
   "metadata": {},
   "source": [
    "2. Feature Importance from Trees:\n",
    "For ensemble methods like Random Forest or Gradient Boosting, you can use the feature importance scores to select the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fdef7-f5a9-4ef8-814a-a3e35bf98148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "model_rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model_rf.feature_importances_\n",
    "\n",
    "# Select top features based on importance scores\n",
    "top_features = X[:, feature_importances.argsort()[-5:][::-1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ada470-6eb3-4c27-8ec8-9bf191f9c7f6",
   "metadata": {},
   "source": [
    "3. L1 Regularization (Lasso):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a2d69-4788-4d62-bd9e-ce62dbeed1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model with L1 regularization\n",
    "model_lasso = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "# Fit the model\n",
    "model_lasso.fit(X, y)\n",
    "\n",
    "# Get selected features (non-zero coefficients)\n",
    "selected_features = X[:, np.abs(model_lasso.coef_)[0] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6b3ee-635c-413e-be5c-54f5634201ac",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing \n",
    "with class imbalance?\n",
    "Ans:-Handling imbalanced datasets in logistic regression is crucial to ensure that the model does not become biased toward the majority class. Here are some strategies for dealing with class imbalance in logistic regression:\n",
    "\r\n",
    "1. Resampling Techniques:\r\n",
    "Oversampling the Minority Clas:\r\n",
    "\r\n",
    "Increase the number of instances in the minority class by duplicating or generating synthetic samples (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\r\n",
    "Undersampling the Majority Class:\r\n",
    "\r\n",
    "Decrease the number of instances in the majority class by randomly removing \n",
    "2. Different Thresholds and Evaluation Metrics:\r\n",
    "Adjust Classification Threshold:\r\n",
    "\r\n",
    "By default, logistic regression uses a threshold of 0.5 for class prediction. Adjust the threshold based on the desired balance between precision and recall.\r\n",
    "Use Appropriate Evaluation Metrics:\r\n",
    "\r\n",
    "Instead of accuracy, use metrics like precision, recall, F1-score, or area under the precision-recall curve (AUC-PR) that are more sensitive to imbalanced cl\n",
    "4. Anomaly Detection:\r\n",
    "Treat the minority class as an anomaly and use anomaly detection techniques.\r\n",
    "5. Ensemble Methods:\r\n",
    "Use ensemble methods like Random Forest or Gradient Boosting, which can handle class imbalance better than individual models.asses.samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b4276-5193-4996-8ab7-6dbdfe67e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic \n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity \n",
    "among the independent variables?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
